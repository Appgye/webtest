<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>高效能 跨媒体关联平台</title>
    <link rel="stylesheet" href="../menu.css">
    <link rel="stylesheet" href="../detail.css">

</head>
<body>

<div class="header">
    <!--    <h1>高效能 跨媒体关联平台</h1>-->
    <div class="headertext">
        高效能 跨媒体关联平台
    </div>

    <!--    可以放一个logo-->
    <!--    移动端适配-->
    <div class="menu-icon" onclick="toggleMenu()">☰ 导航</div>
    <div class="navbar">
        <a href="../../index.html">首页</a>
        <a href="../../dirction/direction1.html">方向1</a>
        <a href="../../dirction/direction2.html">方向2</a>
        <a href="../../dirction/direction3.html">方向3</a>
        <a href="../../dirction/direction4.html">方向4</a>
        <a href="https://cszhengzhang.cn">个人主页</a>
        <a href="https://cszhengzhang.cn/BMI/">课题组主页</a>
    </div>
</div>

<div class="total">

    <div class="kuang">
        <div class="title">
            <h1>Attention is All you Need</h1>
        </div>

        <div class="information">
            期刊：Advances in Neural Information Processing Systems<br>
            作者：Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin<br>
            时间：2017
        </div>
    </div>

    <div class="kuang">
        <div class="pic">
            <!--    <img src="https://via.placeholder.com/300" alt="Image 1" style="margin-right: 10px;">-->
            <!--    <img src="https://via.placeholder.com/300" alt="Image 2">-->
            <img src="../../source/direction1/1-1/1.png" alt="Image 1" style="margin-right: 10px;">
            <img src="../../source/direction1/1-1/2.png" alt="Image 2">
        </div>
    </div>

    <div class="kuang">
        <div class="abstract">
            <h2>摘要</h2>
            <p>The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms. We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.</p>
        </div>
    </div>
</div>


<div class="footer">
    <div class="column">
        <p style="font-size: 24px; font-weight: lighter;">高效能 跨媒体关联平台</p>
        <p>Dr Zheng Zhang @ HITSZ</p>
    </div>
    <div class="column">
        <p style="font-size: 18px;">联系方式</p>
        <p>个人主页：<a href="https://cszhengzhang.cn">cszhengzhang.cn</a></p>
        <p>课题组主页：<a href="https://cszhengzhang.cn/BMI/">cszhengzhang.cn/BMI</a></p>
    </div>
</div>

</body>


<script src="../menu.js"></script>


</html>